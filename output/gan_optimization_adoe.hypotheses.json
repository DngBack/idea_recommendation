{
  "gaps": [
    {
      "id": "gap_1",
      "description": "Existing GAN game optimizers address cycling/instability with isolated mechanisms (extragradient, optimism, or Jacobian/consensus regularization), but there is no well-studied, unified method that explicitly and adaptively damps the rotational (imaginary-eigenvalue) component while retaining fast progress when dynamics are well-behaved.",
      "related_entries": [
        "Training GANs with Optimism",
        "Reducing Noise in GAN Training with Variance Reduced Extragradient",
        "The Numerics of GANs",
        "Towards a Better Understanding and Regularization of GAN Training Dynamics",
        "On Convergence and Stability of GANs"
      ],
      "priority": "high"
    },
    {
      "id": "gap_2",
      "description": "There is a lack of validated, cheap online diagnostics for detecting rotational dynamics/limit cycles during GAN training that can drive an adaptive regularization or damping schedule without Jacobian/Hessian computation.",
      "related_entries": [
        "The Numerics of GANs",
        "Towards a Better Understanding and Regularization of GAN Training Dynamics",
        "Training GANs with Optimism"
      ],
      "priority": "high"
    },
    {
      "id": "gap_3",
      "description": "The interaction between game-dynamics-aware stabilization (optimism/extragradient/regularization) and practical optimizer choices (Adam/Adam variants, momentum) and TTUR learning-rate ratios is under-characterized; it remains unclear which combinations improve stability without increasing sensitivity to hyperparameters.",
      "related_entries": [
        "Training GANs with Optimism",
        "The Numerics of GANs",
        "Towards a Better Understanding and Regularization of GAN Training Dynamics"
      ],
      "priority": "high"
    },
    {
      "id": "gap_4",
      "description": "Empirical comparisons of stabilization methods are often not compute-normalized (wall-clock, steps-to-target, extra forward/backward passes); the per-step overhead of lookahead/extragradient and any Jacobian-vector proxies is rarely evaluated against achieved stability gains.",
      "related_entries": [
        "Reducing Noise in GAN Training with Variance Reduced Extragradient",
        "The Numerics of GANs",
        "Towards a Better Understanding and Regularization of GAN Training Dynamics"
      ],
      "priority": "medium"
    },
    {
      "id": "gap_5",
      "description": "Robustness of rotational-dynamics mitigation under high stochasticity (small batch, limited data, heavy augmentations) is not systematically studied, despite evidence that gradient noise can prevent convergence of standard game optimizers; how adaptive damping should respond to noise vs true rotation is unclear.",
      "related_entries": [
        "Reducing Noise in GAN Training with Variance Reduced Extragradient",
        "Data-Efficient GAN Training Beyond (Just) Augmentations: A Lottery Ticket Perspective",
        "Diffusion-GAN: Training GANs with Diffusion"
      ],
      "priority": "medium"
    },
    {
      "id": "gap_6",
      "description": "Theory and empirically testable predictions for last-iterate stability in nonconvex GANs with adaptive regularization/damping are limited; existing analyses often focus on toy bilinear games or local Jacobian properties without connecting to practical adaptive schedules.",
      "related_entries": [
        "Training GANs with Optimism",
        "The Numerics of GANs",
        "Towards a Better Understanding and Regularization of GAN Training Dynamics",
        "On Convergence and Stability of GANs"
      ],
      "priority": "medium"
    }
  ],
  "hypotheses": [
    {
      "name": "adaptive_damped_optimistic_extragradient_reduces_limit_cycles",
      "short_hypothesis": "For standard image GANs (e.g., CIFAR-10/CelebA with hinge or WGAN-GP losses), an Adaptive Damped Optimistic Extragradient (ADOE) update will reduce oscillatory behavior (measured by gradient-direction cycling metrics) and improve training stability across seeds compared to (i) extragradient alone and (ii) optimism alone, at equal or lower steps-to-target FID.",
      "linked_gap_ids": [
        "gap_1",
        "gap_6"
      ],
      "rationale": "It directly tests the proposed mechanism (explicitly damping rotation) against the closest known baselines (EG, OMD) using measurable stability and sample-quality outcomes."
    },
    {
      "name": "gradient_disagreement_is_a_reliable_rotation_proxy",
      "short_hypothesis": "A cheap proxy based on consecutive-step gradient disagreement (e.g., 1−cos(g_t,g_{t-1}) or sign-flip rate) will positively correlate with Jacobian-imaginary-dominance/rotation indicators (approximated via occasional Jacobian-vector products) and can be used to trigger adaptive damping that improves stability without degrading final FID.",
      "linked_gap_ids": [
        "gap_2",
        "gap_4"
      ],
      "rationale": "If the proxy correlates with rotation and drives a useful controller, it enables practical adaptive damping without expensive curvature estimation, addressing a key feasibility barrier."
    },
    {
      "name": "adoe_reduces_ttur_and_adam_hyperparameter_sensitivity",
      "short_hypothesis": "Under wide sweeps of TTUR ratios (η_g/η_d) and optimizer settings (Adam β parameters), ADOE will exhibit lower variance in achieved FID/IS and fewer divergences than Adam+TTUR baselines, indicating reduced sensitivity to learning-rate ratio tuning.",
      "linked_gap_ids": [
        "gap_3",
        "gap_1"
      ],
      "rationale": "Hyperparameter brittleness is a primary practical failure mode in GANs; showing robustness to TTUR/Adam choices would be a strong, testable advantage tied to controlling rotational dynamics rather than relying on optimizer luck."
    },
    {
      "name": "adaptive_lambda_outperforms_best_fixed_lambda_on_compute_normalized_metrics",
      "short_hypothesis": "An adaptive damping schedule λ_t driven by an oscillation proxy will achieve better compute-normalized performance (wall-clock to reach a target FID, and stability across seeds) than the best fixed λ selected by a tuning budget matched in total training compute.",
      "linked_gap_ids": [
        "gap_4",
        "gap_2"
      ],
      "rationale": "This isolates whether adaptivity itself (not just adding regularization) provides a real efficiency advantage when tuning cost and per-step overhead are properly accounted for."
    },
    {
      "name": "adoe_plus_variance_reduction_improves_small_batch_training",
      "short_hypothesis": "In small-batch or limited-data regimes (high stochastic gradient noise), combining ADOE with a variance-reduced gradient estimator (SVRE-style) will reduce mode collapse indicators and improve stability/FID versus either method alone, while keeping overhead manageable.",
      "linked_gap_ids": [
        "gap_5",
        "gap_1",
        "gap_4"
      ],
      "rationale": "Rotation control and noise control target different failure modes (cycling vs stochastic disruption); testing additivity/interaction is directly actionable for constrained-compute, noisy training settings."
    }
  ]
}